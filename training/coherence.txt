unzip coherence into: /u01/coherence

install java 1.4.2_19 into /u01/j2sdk1.4.2_19

prepend /u01/j2sdk1.4.2_19/bin to root's path, edit /root/.bash_profile with:
PATH=$PATH:$HOME/bin:/u01/j2sdk1.4.2_19/bin

http://wls1032.ftravers.com:7001/ft/untitled.jsp

launch coherence: cd /u01/coherence/lib; java -jar coherence.jar

==> Firewalls
*** This is not resolved, I don't know how to setup the firewall so that it will work properly.

> /usr/bin/system-config-securitylevel-tui
> 33389:udp 33389:tcp 8088:udp 8088:tcp
> service iptables restart

224.3.5.3:35465

Table 1–1 Network Addresses and Ports Used by Coherence 
Address / Port / Type Purpose 
224.3.3.1 / 33389 / Multicast Cluster member discovery and broadcast 
localhost / 8088+ / Unicast Inter-process communication between cluster members. 
(localhost is the local IP address and not the loop 
back address.)

=> Coherence command line commands:
	> cache mycache: Connect to a named cache.
	> get message: retrieve a value stored with the key "message"
	> put message "hello": store a string value with the key "message"
	> list: display the whole list
	> size: return the size (length) of the list
	> remove message: remove the value stored with the key "message"
	> bye: disconnect from a named cache.

=> Coherence Java API
CacheFactory.ensureCluster(); 
This method ensures that you are connected to a cluster, and therefore can safely get a namedCache and read and write values to the cache.

NamedCache myCache = CacheFactory.getCache("mycache");

CacheFactory.shutdown();
This disconnects you from a cache.

=> Coherence Notes

	=> Serialization:
As an alternative to using the Java Serializable interface, you can improve
performance by using Coherence’s own class for high-performance serialization,
com. tangosol.io.pof.PortableObject. PortableObject is up to six times faster
than the standard java.io.Serializable and the serialized result set is
smaller.

	=> Serialization 2:
Note: Cache keys and values must be serializable (for example,
java.io.Serializable). Cache keys must also provide an implementation of the
hashCode() and equals() methods, and those methods must return consistent
results across cluster nodes. This implies that the implementation of
hashCode() and equals() must be based solely on the object's serializable
state (that is, the object's non-transient fields); most built-in Java types,
such as String, Integer and Date, meet this requirement. Some cache
implementations (specifically the partitioned cache) use the serialized form
of the key objects for equality testing, which means that keys for which
equals() returns true must serialize identically; most built-in Java types
meet this requirement as well.

NOTE: To use POF serialization, you must register your user-defined objects in
a POF configuration file.

	=> Bulk loading the cache:
Until now, you have been putting and retrieving objects individually. Each
put can result in increased network traffic, especially for partitioned and
replicated caches. Additionally, each call to put returns the object it just
replaced in the cache, which adds more unnecessary overhead. Loading the cache
can be made much more efficient by using the putAll method instead.



--------------------------------------------------
Questions:
--------------------------------------------------
> Background:
Configuring the Near Cache 
A Near Cache is configured by using the <near-scheme> element in the 
coherence-cache-config file. This element has two required sub-elements: 
front-scheme for configuring a local (front-tier) cache and a back-scheme for defining a 
remote (back-tier) cache. While a local cache (<local-scheme>) is a typical choice for 
the front-tier, you can also use schemes based on Java Objects (<class-scheme>) 
and, other than for .Net and C++ clients, non-JVM heap-based caches 
(<external-scheme> or <paged-external-scheme>).
> Question:
What are the implications for .Net based clients here? They cannot use
class-scheme and should use non-JVM heap-based caches?




--------------------------------------------------
Glossary:
--------------------------------------------------

> Back Cache
	The "back cache" can be a centralized or multi-tiered cache that can
load-on-demand in case of local cache misses. The "back cache" is assumed to
be complete and correct in that it has much higher capacity, but more
expensive in terms of access speed. For the "back cache", a remote,
partitioned cache is used.

> Cache Servers
	The local storage enabled option allows some cluster nodes to be used just for
storing the cache data; such cluster nodes are called Coherence cache servers.
Cache servers are commonly used to scale up Coherence's distributed query
functionality.

> CacheStore
	A CacheStore is an application-specific adapter used to connect a cache to a
underlying data source. The CacheStore implementation accesses the data source
by using a data access mechanism (for example, Hibernate, Toplink Essentials,
JPA, application-specific JDBC calls, another application, mainframe, another
cache, and so on). The CacheStore understands how to build a Java object using
data retrieved from the data source, map and write an object to the data
source, and erase an object from the data source.

> coherence-cache-config
	XML config file.

> Coherency 
	Coherency refers to the fact that all servers will end up with the same
"current" value, even if multiple updates occur at the same exact time from
different servers.

> Distributed Cache Service
	This is the distributed cache service, which allows cluster nodes to
distribute (partition) data across the cluster so that each piece of data in
the cache is managed (held) by only one cluster node. The Distributed Cache
Service supports pessimistic locking. Additionally, to support failover
without any data loss, the service can be configured so that each piece of
data will be backed up by one or more other cluster nodes. Lastly, some
cluster nodes can be configured to hold no data at all; this is useful, for
example, to limit the Java heap size of an application server process, by
setting the application server processes to not hold any distributed data, and
by running additional cache server JVMs to provide the distributed cache
storage.

> <distributed-scheme>


> Front Cache
	The typical deployment uses a Local Cache for the "front cache". A Local
Cache is a reasonable choice because it is thread safe, highly concurrent,
size-limited and/or auto-expiring and stores the data in object form. For the
"back cache", a remote, partitioned cache is used. The use of a Near Cache is
not confined to Coherence*Extend; it also works with TCMP.

> Java NIO
	New I/O or Non-blocking I/O, usually called NIO, is a collection of Java
programming language APIs that offer features for intensive I/O operations.

> Local Storage Enabled
	The Java heap size of the cluster nodes that have turned off local storage
enabled will not be affected at all by the amount of data in the cache,
because that data will be cached on other cluster nodes. This is particularly
useful for application server processes running on older JVM versions with
large Java heaps, because those processes often suffer from garbage collection
pauses that grow exponentially with the size of the heap.

> Local Cache
	While it is not a clustered service, the Coherence local cache implementation
is often used in combination with various Coherence clustered cache services.
The Coherence local cache is just that: a cache that is local to (completely
contained within) a particular cluster node.
	The key element for configuring the Local Cache is <local-scheme>
	If a <cache-store-scheme> is not specified, then the cached data will only
reside in memory, and only reflect operations performed on the cache itself.

> NamedCache
	a hosted NamedCache corresponds roughly to a database table.

> Near Cache
	The objective of a Near Cache is to provide the best of both worlds between
the extreme performance of the Replicated Cache Service and the extreme
scalability of the Partitioned Cache Service by providing fast read access to
Most Recently Used (MRU) and Most Frequently Used (MFU) data. To achieve this,
the Near Cache is an implementation that wraps two caches: a "front cache" and
a "back cache" that automatically and transparently communicate with each
other by using a read-through/write-through approach.
	The current release of Coherence also enables you to configure a Near Cache
for Java, C++, or for .NET clients.

> Near Cache Invalidation Strategies
	> None: 
		This strategy instructs the cache not to listen for invalidation 
	events at all. This is the best choice for raw performance and 
	scalability when business requirements permit the use of data 
	which might not be absolutely current. Freshness of data can be 
	guaranteed by use of a sufficiently brief eviction policy for the 
	front cache.
	> Present: 
		This strategy instructs the Near Cache to listen to the back cache 
	events related only to the items currently present in the front 
	cache. This strategy works best when each instance of a front 
	cache contains distinct subset of data relative to the other front 
	cache instances (for example, sticky data access patterns).
	> All: 
		This strategy instructs the Near Cache to listen to all back cache 
	events. This strategy is optimal for read-heavy tiered access 
	patterns where there is significant overlap between the different 
	instances of front caches.
	> Auto: 
		This strategy instructs the Near Cache to switch automatically 
	between Present and All strategies based on the cache 
	statistics.

> Partitioned The data in a distributed cache is spread out over all the
servers in such a way that no two servers are responsible for the same piece
of cached data. This means that the size of the cache and the processing power
associated with the management of the cache can grow linearly with the size of
the cluster. Also, it means that operations against data in the cache can be
accomplished with a "single hop," in other words, involving at most one other
server.

> Partitioned Cache
	To address the potential scalability limits of the replicated cache service,
both in terms of memory and communication bottlenecks, Coherence has provided
a distributed cache service since release 1.2. Many products have used the
term distributed cache to describe their functionality, so it is worth
clarifying exactly what is meant by that term in Coherence. Coherence defines
a distributed cache as a collection of data that is distributed (or,
partitioned) across any number of cluster nodes such that exactly one node in
the cluster is responsible for each piece of data in the cache, and the
responsibility is distributed (or, load-balanced) among the cluster nodes.

	Partitioned (Distributed) cache service in Coherence has three distinct
layers:
	> Client View: 	
		The client view represents a virtual layer that provides
	access to the underlying partitioned data. Access to this tier is provided
	using the NamedCache interface. In this layer you can also create synthetic
	data structures such as NearCache or ContinuousQueryCache.
	> Storage Manager:
		The storage manager is the server-side tier that is responsible 
	for processing cache-related requests from the client tier. It manages the data 
	structures that hold the actual cache data (primary and backup copies) and 
	information about locks, event listeners, map triggers etc. 
	> Backing Map:
		The Backing Map is the server-side data structure that holds actual 
	data. 



> Read-Through
	When an application asks the cache for an entry, for example the key X, and X
is not already in the cache, Coherence will automatically delegate to the
CacheStore and ask it to load X from the underlying data source. If X exists
in the data source, the CacheStore will load it, return it to Coherence, then
Coherence will place it in the cache for future use and finally will return X
to the application code that requested it. This is called Read-Through
caching.

> Replicated Cache
	The replicated cache excels in its ability to handle data replication,
concurrency control and failover in a cluster, all while delivering in-memory
data access speeds. A clustered replicated cache is exactly what it says it
is: a cache that replicates its data to all cluster nodes.
	The limitations of the replicated cache service should also be carefully
considered. First, however much data is managed by the replicated cache
service is on each and every cluster node that has joined the service. That
means that memory utilization (the Java heap size) is increased for each
cluster node, which can impact performance. Secondly, replicated caches with a
high incidence of updates will not scale linearly as the cluster grows; in
other words, the cluster will suffer diminishing returns as cluster nodes are
added.

> Replicated Cache Service
	This is the synchronized replicated cache service, which fully replicates all
of its data to all cluster nodes that run the service. Furthermore, it
supports pessimistic locking so that data can be modified in a cluster without
encountering the classic missing update problem. With the introduction of near
caching and continuous query caching, almost all of the functionality of
replicated caches is available on top of the Distributed cache service (and
with better robustness). But replicated caches are often used to manage
internal application metadata.

> <remote-cache-scheme>
	The <remote-cache-scheme> element enables you to use a clustered cache from
outside the current cluster.

> Service Instance
	By way of analogy, a service instance corresponds roughly to a database
schema, and in the case of data services, a hosted NamedCache corresponds
roughly to a database table.


> Write-Behind
	In the Write-Behind scenario, modified cache entries are asynchronously
written to the data source after a configurable delay, whether after 10
seconds, 20 minutes, a day or even a week or longer. For Write-Behind caching,
Coherence maintains a write-behind queue of the data that must be updated in
the data source. When the application updates X in the cache, X is added to
the write-behind queue (if it isn't there already; otherwise, it is replaced),
and after the specified write-behind delay Coherence will call the CacheStore
to update the underlying data source with the latest state of X. Note that the
write-behind delay is relative to the first of a series of modifications—in
other words, the data in the data source will never lag behind the cache by
more than the write-behind delay.

> write-coalescing
	The writes, which are typically much more expensive operations, are often
reduced because multiple changes to the same object within the write-behind
interval are "coalesced" and only written once to the underlying data source
("write-coalescing").

> write-combining
	Additionally, writes to multiple cache entries may be combined into a single
database transaction ("write-combining") by using the CacheStore.storeAll()
method.

> Write-Through
	Coherence can handle updates to the data source in two distinct ways, the
first being Write-Through. In this case, when the application updates a piece
of data in the cache (that is, calls put(...) to change a cache entry,) the
operation will not complete (that is, the put will not return) until Coherence
has gone through the CacheStore and successfully stored the data to the
underlying data source. This does not improve write performance at all, since
you are still dealing with the latency of the write to the data source.
Improving the write performance is the purpose for the Write-Behind Cache
functionality. See "Write-Behind Caching" for more information.
	The application experiences drastically reduced database load: Since the
amount of both read and write operations is reduced, so is the database load.
The reads are reduced by caching, as with any other caching approach. The
writes, which are typically much more expensive operations, are often reduced
because multiple changes to the same object within the write-behind interval
are "coalesced" and only written once to the underlying data source
("write-coalescing"). Additionally, writes to multiple cache entries may be
combined into a single database transaction ("write-combining") by using the
CacheStore.storeAll() method.
