* system setup
  
: yaourt intel-mkl-slim

The non-slim variety of above dies, maybe because /tmp 4g drive got
full with an tgz extraction?

dont know about following:

: yaourt intel-opencl-sdk

** GPU

https://wiki.archlinux.org/index.php/GPGPU

dell desktop:

: yaourt intel-opencl-runtime

samsung laptop:

pacman -S beignet

* Learning

* Dragan Rocks

https://dragan.rocks/

** tutorial - Fast, Native Speed, Vector Computations in Clojure

https://neanderthal.uncomplicate.org/articles/tutorial_native.html


* 7 Gate
** pre-start links
*** TODO [[https://www.youtube.com/playlist?list=PLAwxTw4SYaPl0N6-e1GvyLp5-MUMUjOKo
*** TODO [[https://www.youtube.com/playlist?list=PLAwxTw4SYaPl0N6-e1GvyLp5-MUMUjOKo][youtube 2 - first 18 videos]]

*** TODO The Harvard CS109 class of 2015 
has [[https://cs109.github.io/2015/pages/videos.html][hands-on examples]] of the above concepts. Work on things you may
have not fully grasped with the below material

The following sessions address the general tooling such as using the command line,
Python (NumPy, Matplotlib, Pandas, Seaborn), Jupyter Notebooks, Git (and GitHub),
and sending HTTP requests. You must be comfortable with these before attending
the classes. The following sessions may assist that:

*** TODO a. [[https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=e15f221c-5275-4f7f-b486-759a7d483bc8][Lab1 about Jupyter and Git]] 

(Note that Jupyter Notebook has evolved into
Jupyter Lab since the sessions were recorded and we will be using the latter
in the class.
*** TODO [[https://nbviewer.jupyter.org/github/johannesgiorgis/school_of_ai_vancouver/blob/master/intro_to_data_science_tools/01_introduction_to_conda_and_jupyter_notebooks.ipynb][A more updated introductory source is available here]])

*** TODO b. [[https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=f7ff1893-fbf7-4909-b44e-12e61a98a677][Lecture 2 for Pandas]] (scraping part optional)

*** TODO c. [[https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=f8a832cb-56e7-401b-b485-aec3c9928069][Lecture 4 Databases]]

2. The following sessions are concept refreshers on cohort prerequisites:

*** TODO a. [[https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=8af4418a-7f5b-4738-9c72-6fc2ba1fc499][Lab 3: Probability and distributions]]

*** TODO b. [[https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=afe70053-b8b7-43d3-9c2f-f482f479baf7][Lecture 7: Bias]]

*** TODO c. Lab 4 on Regression [[https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=483c8b93-3700-4ee8-80ed-aad7f3da7ac2][video]] and [[https://github.com/cs109/2015lab][notebook]]

*** TODO Step 3: Data Science Presentations
Study [[https://drive.google.com/drive/folders/1e3OYZn_0VAGLEClJYJZ0OoJvy6Qj-dsi][academyâ€™s pre-course presentations]] and make sure you search online for any
concepts that you are not familiar with.
* jupyter, python, etc

Start Jupyter Lab:

: jupyter-lab 

** python: numpy, matplotlib, pandas, seaborn

#+BEGIN_SRC python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
#+END_SRC

* ML Notes

** Linear Regression - AKA least squares

** Linearly Separable

You can draw a line between two sets of data

** KNN - x nearest neighbors
just save all the data into the database and future queries lookup to
closest value and k neighbours to figure out what answer should be.

** Cross Validation

Shuffle the dataset randomly.
Split the dataset into k groups
For each unique group:
Take the group as a hold out or test data set
Take the remaining groups as a training data set
Fit a model on the training set and evaluate it on the test set
Retain the evaluation score and discard the model
Summarize the skill of the model using the sample of model evaluation scores
** Bias 
High bias means more error in your predictions.
** Cost/Loss/Objective function
how far we are away from the right answer.
** gradient descent
*** learning rate
The size of the step we take is called the learning rate. 
* jupyter, python, etc

Start Jupyter Lab:

#+BEGIN_SRC python
import pandas as pd
# tab separated, column 0 is an index column
the_data = pd.read_csv("mydata.tsv", sep="\t", index_col=0)

# get min, max, std, mean info about the data
the_data.describe()

# get the column names (NOTE: property not function):
the_data.columns

# general info about the data, like how much memory it uses:
the_data.info()

# retrieve the first rows of the data to have a look at it
the_data.head()
#+END_SRC

** Modeling

This is following the kaggle intro to machine learning tutorial

https://www.kaggle.com/dansbecker/your-first-machine-learning-model

First lets select the thing we want to predict

#+BEGIN_SRC python
y = the_data.hearing_damage
#+END_SRC

Choosing ~features~ that are considered as inputs to the predictions
We select multiple features by providing a list of column names inside
brackets.

#+BEGIN_SRC python
the_data_features = ['source_intensity', 'horizontal_distance', 'vertical_distance', 'insulation']
#+END_SRC

Now lets get a pandas dataframe with just these columns:

#+BEGIN_SRC python
X = the_data[the_data_features]

from sklearn.tree import DecisionTreeRegressor

# Define model. Specify a number for random_state to ensure same results each run
data_model = DecisionTreeRegressor(random_state=1)

data_model.fit(X, y)

data_model.predict( X.head())

X.head()
#+END_SRC

Lets check how good the model is:

#+BEGIN_SRC python
from sklearn.metrics import mean_absolute_error

predicted_home_prices = melbourne_model.predict(X)
mean_absolute_error(y, predicted_home_prices)
#+END_SRC

** split up the data
   
Its not good to use same data to train AND test with.

#+BEGIN_SRC python
from sklearn.model_selection import train_test_split

# run this script.
train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)

# Define model
data_model = DecisionTreeRegressor()

# Fit model
data_model.fit(train_X, train_y)

# get predicted prices on validation data
val_predictions = data_model.predict(val_X)

print(mean_absolute_error(val_y, val_predictions))
#+END_SRC

** vary decision tree depth

We can adjust the depth of the decision tree with a line like:

#+BEGIN_SRC python
model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)
#+END_SRC

We can test a variety of tree depths and their MAE with:

#+BEGIN_SRC python
def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):
    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)
    model.fit(train_X, train_y)
    preds_val = model.predict(val_X)
    mae = mean_absolute_error(val_y, preds_val)
    return(mae)

for max_leaf_nodes in [5, 50, 500, 5000]:
    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)
    print("Max leaf nodes: %d  \t\t Mean Absolute Error:  %d" %(max_leaf_nodes, my_mae))
    
** random forests

#+BEGIN_SRC python
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error

forest_model = RandomForestRegressor(random_state=1)
forest_model.fit(train_X, train_y)
melb_preds = forest_model.predict(val_X)
print(mean_absolute_error(val_y, melb_preds))
#+END_SRC


#+END_SRC
* pandas
** dataframes
#+BEGIN_SRC python
# initialize list of lists 
data = [['tom', 10], ['nick', 15], ['juli', 14]] 
  
# Create the pandas DataFrame 
df = pd.DataFrame(data, columns = ['Name', 'Age']) 
#+END_SRC

* Inference
For now, though, we are only dealing with inference, the process of
computing the output using the given structure, input, and whatever
weights there are.

* functions
** uncomplicate.neanderthal.native
    
dge rows columns
Creates a GE matrix using double precision floating point native CPU engine

dv
Creates a vector using double precision floating point native CPU engine

** uncomplicate.neanderthal.core
(axpy! alpha x y): a times x plus y destructive.
multiplies elements of vector/matrix ~x~ by scalar ~alpha~, then adds
the result to vector/matrix ~y~.

mm! - matrix-matrix multiplication
(mm! alpha a b)
(mm! alpha a b c)
(mm! alpha a b beta c)
Multiply matrix a by b.  Scale by alpha. Put result into one of a/b
whichever is the GE matrix.  If ~c~ is supplied result is put there.
If scalar ~beta~ is supplied first scale ~c~ by it. 

(mrows a)
Returns the number of rows of the matrix ~a~.

mv! - Matrix-Vector multiplication
(mv! m1 x1 y)
Multiplies matrix ~m1~, by vector ~x1~, and adds it to vector ~y~.

(ncols a)
Returns the number of columns of the matrix ~a~.

rk!
(rk! alpha x y a)
Multiplies vector ~x~ with transposed vector ~y~, scales resulting matrix
by ~alpha~, add result to ~a~.

** uncomplicate.neanderthal.vect-math

fmax - keep max value of each pair from 2 vectors
  (let [v1 (dv [1 2 3])
        v2 (dv [0 2 7])]
    (fmax v1 v2)) ;; => [1.00 2.00 7.00]
* kaggle tutorial
